# Rag_Portfolio
# RAG Portfolio App

A fully local-first, free Retrieval-Augmented Generation (RAG) system that lets you upload documents (PDF, DOCX, PPTX, XLSX, MD), semantically search them, and chat with an LLM (via Groq API) over their content.

---

## ğŸ”§ Features

* âœ… Upload multi-format documents
* âœ… Extract, chunk, and embed using MiniLM
* âœ… Store embeddings in local FAISS vector DB
* âœ… Chat UI with memory and streaming responses
* âœ… LLM-powered Q\&A via Groq (e.g., gemma2-9b-instruct)
* âœ… No cloud storage or paid components required

---

## ğŸ“¦ Tech Stack

| Layer      | Tool                           |
| ---------- | ------------------------------ |
| UI         | Streamlit                      |
| Backend    | Python                         |
| Extractors | pdfplumber, python-docx, etc.  |
| Chunking   | LangChain                      |
| Embedding  | sentence-transformers (MiniLM) |
| Vector DB  | FAISS                          |
| LLM        | Groq API (OpenAI-compatible)   |

---

## ğŸ“ Project Structure

```
rag-portfolio-app/
â”œâ”€â”€ app.py                 # Streamlit app
â”œâ”€â”€ .env                  # Groq API key + model
â”œâ”€â”€ documents/            # Uploaded files
â”œâ”€â”€ vector.index          # FAISS index (persisted)
â”œâ”€â”€ meta.pkl              # Chunk metadata (persisted)
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ parser.py         # File extractors
â”‚   â”œâ”€â”€ chunker.py        # Text chunker
â”‚   â”œâ”€â”€ embedding_store.py# VectorDB manager
â”‚   â””â”€â”€ groq_client.py    # LLM call logic
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Setup Instructions

```bash
# 1. Clone & enter project folder
$ git clone <repo-url> && cd rag-portfolio-app

# 2. Create and activate virtual env
$ python -m venv venv
$ source venv/bin/activate  # or venv\Scripts\activate on Windows

# 3. Install dependencies
$ pip install -r requirements.txt

# 4. Add your Groq credentials to `.env`
GROQ_API_KEY=sk-xxx
GROQ_MODEL=gemma2-9b-instruct

# 5. Launch app
$ streamlit run app.py
```

---

## ğŸ’¬ How It Works

1. Upload one or more supported documents
2. They're parsed â†’ chunked â†’ embedded â†’ stored in FAISS
3. Ask a question in the chat
4. Top-k matching chunks retrieved
5. Query + history + context sent to Groq LLM
6. Answer returned and shown in chat with memory

---

## ğŸ§  Advanced Features

* ğŸ”„ Persistent vector store (`vector.index`, `meta.pkl`)
* ğŸ’¬ Conversational memory via `st.session_state`
* ğŸ“Œ Prompt combines history + document context
* âš¡ Supports multi-doc queries seamlessly
* ğŸ” .env-based secrets management

---

## ğŸ”’ Security / Limitations

* No real-time Notion sync (manual .md only)
* No user login/session isolation (MVP)
* Chat not persisted unless you add a DB or `.json` log

---

## ğŸ‘¨â€ğŸ’» Author

Built with â¤ï¸ by Rajarshi

---

## âœ¨ Sample Prompt (to LLM)

```text
You are an assistant. Use the following conversation history and document context to answer the question:

Conversation History:
User: What is a jailbreak prompt?
Assistant: A prompt that bypasses the safeguards of an LLM...

Retrieved Document Context:
...

Current Question:
How do jailbreak prompts evolve over time?

Answer:
```

---
